### プロローグ1~4  
#### 識別モデルと生成モデル
識別モデル - データを目的のクラスに分類する。画像をもとに、どのクラスにわけられそうか、を確率で考える。  
生成モデル - 特定のクラスのデータを生成する。文字などの入力をもとに、どのピクセルに色をつければそれになりそうかを確率で考える。  
![image](https://user-images.githubusercontent.com/20613753/139427260-243d3a83-4b33-4720-ac6c-adbaa7ce3639.png)  

#### 識別器
![image](https://user-images.githubusercontent.com/20613753/139427425-77aaf620-6564-4448-961a-3d419a86aebc.png)  
ベイズの定理により生成モデルを使って、識別モデルを作成することもできる。  
識別関数は、確率ではなく0,1で出力する。  

識別モデル  
![image](https://user-images.githubusercontent.com/20613753/139434340-8bea0d74-7f33-4f57-a928-bf89899413f4.png)  
  
生成モデル  
![image](https://user-images.githubusercontent.com/20613753/139434657-c700cc88-29f5-4827-baa5-b4b332b0d8ae.png)  

#### 万能近似定理
非線形な活性化関数を使うことで、どんな関数でも近似化できる

### 01ニューラルネットワークの全体像
入力層、中間層、出力層の3層に分かれる

#### ニューラルネットワークでできること
回帰と分類  
![image](https://user-images.githubusercontent.com/20613753/139438431-c5253760-fd30-4404-8756-fbf11c5cabfe.png)  
![image](https://user-images.githubusercontent.com/20613753/139438451-54cbc0ae-17a1-42aa-9cce-1124fd8663cd.png)
  
数字を入力して、数字を出力するという思想。  
文章を入出力するものは、それらを数字に変換して処理している。

### 02入力層～中間層
1次関数の傾きと切片を求めることが目的。(傾き=w、切片=b)  
![image](https://user-images.githubusercontent.com/20613753/139520309-9a708d43-0ee1-47ff-acbf-80b587cd66fb.png)  

![image](https://user-images.githubusercontent.com/20613753/139522252-11194e30-18ec-4647-9cfd-08020288e8cf.png)  
↓コードで書くとこうなる  
![image](https://user-images.githubusercontent.com/20613753/139522304-4f516378-da22-4a95-b3fe-cd9603d8e05c.png)  

![image](https://user-images.githubusercontent.com/20613753/139522322-293a00b2-0654-4612-ac9c-7bda720465e7.png)  
↓コードで書くとこうなる  
![image](https://user-images.githubusercontent.com/20613753/139522347-7576d736-15d0-4837-9a78-a8b13fe5ff2a.png)  

### 03活性化関数
活性化関数→非線形の関数  
入力層の結果に活性化関数を通すことで、複雑な関数を作ることができる  

活性化関数の種類
ステップ関数-今はあまり使われない  
![image](https://user-images.githubusercontent.com/20613753/139523109-b003b6e3-fcdb-46ad-9eef-612c78dc43e3.png)  
  
シグモイド関数-勾配消失問題  
![image](https://user-images.githubusercontent.com/20613753/139523122-5479118d-8729-4b95-babe-e1a0c94ba549.png)  
  
RELU関数-勾配消失問題が比較的起こりにくい  
![image](https://user-images.githubusercontent.com/20613753/139523144-3c3a96f4-c0f2-4c55-80b2-c8a1f25a5a66.png)  


【実装演習結果】
![image](https://user-images.githubusercontent.com/20613753/139523478-ed71b923-3254-4024-962e-03da9bc27125.png)  
![image](https://user-images.githubusercontent.com/20613753/139523489-0e1284f7-c659-45cf-b466-55cb0de48873.png)  
![image](https://user-images.githubusercontent.com/20613753/139523502-c0388b13-729d-4f32-a968-434ea82f04f3.png)  
![image](https://user-images.githubusercontent.com/20613753/139523551-0cc87198-05fa-416c-80ff-b9b893b24274.png)  
![image](https://user-images.githubusercontent.com/20613753/139523559-3af34597-20a5-4c5b-9621-4a32e08ab09a.png)  
![image](https://user-images.githubusercontent.com/20613753/139523576-776d1e38-2664-4d3c-9141-dab7685023e4.png)  
![image](https://user-images.githubusercontent.com/20613753/139523586-d05a11ef-4e7e-4123-bd4a-c3be7593dfdb.png)  
![image](https://user-images.githubusercontent.com/20613753/139523609-6cd19b25-260d-4580-934f-f82d51566ebb.png)  
![image](https://user-images.githubusercontent.com/20613753/139523617-d9ae7277-6233-4a67-b377-b507bc1380f1.png)  
![image](https://user-images.githubusercontent.com/20613753/139523633-b381239e-27fa-491b-8a31-8f3a15a00f77.png)  
![image](https://user-images.githubusercontent.com/20613753/139523637-1c3b9fcb-6548-47bd-897d-f1b62dad51c7.png)  
![image](https://user-images.githubusercontent.com/20613753/139523643-1292fa6b-73ea-4b2e-8780-d4f6c6cff919.png)  
![image](https://user-images.githubusercontent.com/20613753/139523650-010473f8-3f4c-4a18-af39-0833d5ab3523.png)  
![image](https://user-images.githubusercontent.com/20613753/139523653-8fd49c2c-c0e3-4fdc-97c5-70db64c9fd59.png)  
![image](https://user-images.githubusercontent.com/20613753/139523655-1bcffaca-7e40-4b78-a008-5944b67dac53.png)  
![image](https://user-images.githubusercontent.com/20613753/139523657-88a4cf9e-b86c-45ec-b0f7-a00e0a790f05.png)  
![image](https://user-images.githubusercontent.com/20613753/139523668-c7fbd48b-24a0-4b04-9f7a-bb82de05f86c.png)  
![image](https://user-images.githubusercontent.com/20613753/139523675-4a2b12f6-2b7d-4b72-8d13-8a25e3c4cab7.png)  
![image](https://user-images.githubusercontent.com/20613753/139523680-9e304aa6-1083-4c36-8dc8-febdfe28b79a.png)  
![image](https://user-images.githubusercontent.com/20613753/139523682-b71cdb17-2698-451d-af09-890d2283315c.png)  
![image](https://user-images.githubusercontent.com/20613753/139523685-4595b289-5d53-42f7-9601-a3b6935d4827.png)

### 04出力層_誤差関数
出力層  
![image](https://user-images.githubusercontent.com/20613753/139523794-11be6523-b99e-4bad-a6e1-855f70cead24.png)  
出力したものと訓練データを比較し、どれくらい正解から外れていたかを誤差関数で計算する。  
  
誤差関数が2乗誤差の場合の式  
![image](https://user-images.githubusercontent.com/20613753/139523911-d4f8cc84-289b-44db-8139-76b3055d69ae.png)  
式の頭についている1/2は、微分する際、2乗であるので、式全体に×2が行われるため。  
（計算が簡単になる。本質的な意味はない）  

### 05出力層_活性化関数
出力層と中間層で使われる活性化関数は異なる。  
出力層では、以下の活性化関数が使われる。  
・恒等写像  
・シグモイド関数  
・ソフトマックス関数  
![image](https://user-images.githubusercontent.com/20613753/139526806-5cb60ae5-b60c-452d-89d3-e8fabd23fda9.png)  
求めたい答えに応じて、使われる活性化関数と誤差関数が異なる。  
![image](https://user-images.githubusercontent.com/20613753/139526840-7ab389eb-c256-445e-9d7b-34313d2aa762.png)  

```
def softmax(x):
    if x.ndim == 2:
        x = x.T
        x = x - np.max(x, axis=0)
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T

    x = x - np.max(x) # オーバーフロー対策
    return np.exp(x) / np.sum(np.exp(x))
```
上記コードの以下の部分が、ソフトマックスの計算を行う箇所  
np.exp(x) / np.sum(np.exp(x))  
exp＝エクスポネンシャル  
  
![image](https://user-images.githubusercontent.com/20613753/139527104-572951ca-836d-4f5f-b803-1e17ea6b421f.png)  

```
def cross_entropy_error(d, y):
    if y.ndim == 1:
        d = d.reshape(1, d.size)
        y = y.reshape(1, y.size)
        
    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換
    if d.size == y.size:
        d = d.argmax(axis=1)
             
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), d] + 1e-7)) / batch_size
```
↑のコードのうち、  
(y[np.arange(batch_size), d] + 1e-7)  
の箇所が、dlogyを計算している  
-np.sum  
の箇所は、シグマの計算をしている箇所。

### 06勾配降下法
誤差を最小化するパラメータwを発見したい。  
→勾配降下法を利用して、それを発見する。  
  
学習率→高すぎると、学習が発散してしまう。低すぎると、極小値で学習が収束してしまう。  
「入力→出力→フィードバック」の流れをエポックという。  
画像の識別をする場合、1クラス1000枚以上の画像データがあると、学習がうまくいくことが多い。   
### 07確率的勾配降下法  
・計算コストの軽減  
・極小解に収束するリスクの軽減  
・オンライン学習ができる  
  
<オンライン学習>  
学習データが入ってくるたびに、都度パラメータを更新し、学習を進めていく方法。  
一方、バッチ学習では、一度にすべての学習データを使ってパラメータ更新を行う。  
→画像データは、大きな容量を扱うため、メモリを大量に消費してしまう。  
小分けにして一度に扱う容量を減らすことでメモリの消費を節約できる  
  
### 08ミニバッチ勾配降下法
![image](https://user-images.githubusercontent.com/20613753/144849317-39031f00-459b-4047-9eff-d484af496dd9.png)  
ミニバッチ勾配降下法は1/Nしている。確率的勾配降下法との違い。  
→CPUのスレッド並列化、GPUのSIMD並列化に対応できる  
  
![image](https://user-images.githubusercontent.com/20613753/144849955-06dd2058-9f24-44e4-b565-ce248cb30dcd.png)  
↑確率的勾配降下法の式  
↓この式の意味
![image](https://user-images.githubusercontent.com/20613753/144849876-4358d326-34a4-4d26-a157-d5c27c405607.png)  
  
  
### 09誤差購買の計算
![image](https://user-images.githubusercontent.com/20613753/144850160-242bab40-be92-431a-bdb5-82caed4ba134.png)  
↑誤差勾配  
  
![image](https://user-images.githubusercontent.com/20613753/144850322-071b02c1-3d60-4d1a-a420-edb1e4a4b2f7.png)
↑数値微分のデメリットは、CPUへの負荷が大きい  
よって、次の誤差逆伝搬法を使って、誤差勾配を求める  

### 10誤差逆伝搬法
微分の連鎖率を利用すると、計算量が少なくて済む。  
![image](https://user-images.githubusercontent.com/20613753/144997377-fb1fa9ab-bb9a-45ab-aba2-86b0b6f18b59.png)  
↑微分する箇所がたくさんある。数値微分だと、それぞれで計算をする必要があったけど、  
誤差逆伝搬法を利用することで、微分の連鎖率で計算量がすくなくて済む。  

![image](https://user-images.githubusercontent.com/20613753/144997845-83dc8509-4f6d-4814-942d-cac483d9a6c6.png)  
↑図はコードでいうと、↓になる  
![image](https://user-images.githubusercontent.com/20613753/144997903-4cbfc93f-08f7-4f29-aa1d-59c322b1895a.png)  
↑のdelta2が↓の図でも使われている。計算結果を再利用することで、計算量を減らすことができる。  
![image](https://user-images.githubusercontent.com/20613753/144998093-5bcf6291-1ecd-4e8b-96d4-235955059fb5.png)  
  
### 11ディープラーニングの開発環境
![image](https://user-images.githubusercontent.com/20613753/144999315-d5430e47-bf6e-4159-8a20-9b5abef7ba62.png)  
  
### 12入力層の設計
![image](https://user-images.githubusercontent.com/20613753/145000544-3c11a87b-adb5-4fa6-8438-3d41ae9a7373.png)  
  
入力層の設計  
![image](https://user-images.githubusercontent.com/20613753/145001455-d6178452-1ac5-49df-adaf-d4d11ad19a00.png)  

### 13過学習
大きなNNだと、過学習が起きやすい。  
↓右の図のような状態だと、過学習が起きていると判断できる。  
![image](https://user-images.githubusercontent.com/20613753/145001790-b404f369-0475-4b63-9f5b-caa12e7e6b6c.png)  
  
### 14データ集合の拡張
Data Augmentation  
→画像のデータを水増しする方法。分類タスク(画像)に効果が高い。密度推定には効果がない。  
  
色んな手法でデータ拡張が可能  
![image](https://user-images.githubusercontent.com/20613753/145002286-ba1e040b-d07c-4871-83ea-3c0d65f80a41.png)  
  
![image](https://user-images.githubusercontent.com/20613753/145002925-47722de9-50ff-4e7d-a4de-dc9835d78bca.png)  

### 15CNNで扱えるデータの種類
CNN→次元間で繋がりがあるデータが扱える(データの順番に意味がある)  
例：音声データ、カラー画像、動画データなど  
  
### 16特徴量の転移
転移学習、ファインチューニングは、学習済みモデルから特徴量を転用できるため、学習のコストを減らすことができる。  
  
