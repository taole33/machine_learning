### 機械学習モデリングプロセス
![image](https://user-images.githubusercontent.com/20613753/138542686-04b750a9-034e-4cab-9063-ffa1507e1945.png)  

### 講義で扱う全体像
![image](https://user-images.githubusercontent.com/20613753/138542757-d97f3ed7-eb29-4c9f-a831-438f618195c8.png)  

### ルールベースと機械学習モデル

### 線形回帰モデル
そもそも線形とは？  
→比例  
y=Ax+B,z=Ax+By+C(2次元、3次元)  

advancedな内容:密度推定  

![image](https://user-images.githubusercontent.com/20613753/138543999-93e2ec39-568c-4414-92d4-3fae0174a52f.png)＝実数  
![image](https://user-images.githubusercontent.com/20613753/138544002-3efac8a5-be8d-4672-91f7-12911dac8532.png)＝ベクトル(太字のx）  

![image](https://user-images.githubusercontent.com/20613753/138544104-c6049f2a-568e-40e8-9151-d510464c11d1.png)  
線形回帰モデルの目的は、上記の式のwを求めること。  

![image](https://user-images.githubusercontent.com/20613753/138544293-816afe1e-c337-44b9-b16b-da9eeaecfaeb.png)  
変数の考慮が漏れていた場合は、すべて誤差として計上される。誤差が大きすぎる場合は、変数の考慮忘れかもしれない。

![image](https://user-images.githubusercontent.com/20613753/138544331-511faf3b-192b-47d0-8a19-40329585a524.png)  
連立方程式⇔行列表現　この行き来ができるかが、ひとつの壁である。

![image](https://user-images.githubusercontent.com/20613753/138544612-5b2b84d8-d0b7-4cfa-96e3-40786f81aa12.png)    
↑のものを計画行列という  

機械学習のエッセンス→オススメの書籍  

### 線形回帰モデル  
線形回帰モデルのパラメータは最小二乗法で推定する  
(advanced：最小二乗法は、外れ値に弱い→データを外れ値を修正する。またはHuber損失、Tukey損失を使用するのも手)  
![image](https://user-images.githubusercontent.com/20613753/138545307-e974fc41-f8d9-4cef-b3da-a3aef3e2d7e6.png)  
![image](https://user-images.githubusercontent.com/20613753/138545291-c5259bec-4642-4bc9-bfd9-4ff5f41bcd7c.png)  

