### 非線形な回帰

![image](https://user-images.githubusercontent.com/20613753/138592145-04aa7083-e1e0-4a44-8792-04ffb5d6b39a.png)  
↑の式は線形だが、xを非線形になるようなΦ(x)に変えれば良い！(Φ(x)=xに関する関数)  
(例えばxをxの二乗、sinxに置き換えるなど)  
  
(w(パラメータ)については非線形にせずとも、xの部分を非線形にすることで非線形な回帰を行うことができる)  
(wは求めるべき値だが、求める値は線形で良い)  
  
![image](https://user-images.githubusercontent.com/20613753/138592296-f9e1779c-c505-422d-9b21-875a848b2c1b.png)  
↑ここの部分を基底関数という  

![image](https://user-images.githubusercontent.com/20613753/138592578-49fcc078-5973-4261-82e3-ea79ba5fd702.png)  
↑非線形な回帰といえども、線形と同じ式で解ける(↓と同じ)  

![image](https://user-images.githubusercontent.com/20613753/138592601-54c1cc78-c1af-4421-8363-106dc9da2bfa.png)  
(テキスト26p)  

### 未学習と過学習
・未学習への対応　→　表現力の高いモデルを利用する

・過学習への対応　→　学習データの数を増やす、不要な変数を削除して表現力を抑止、正則化法(罰則化法)を利用する  
![image](https://user-images.githubusercontent.com/20613753/138594776-5e30e0f6-0af9-48b1-ba92-7d0e8ebcea91.png)  
L2ノルムを使用→Ridge推定量  
L1ノルムを使用→Lasso推定量  
  
![image](https://user-images.githubusercontent.com/20613753/138594668-e7d34265-ee1d-455a-ac6c-5b0ad04caf1a.png)  
↑正則化することで、規定関数が増えても、過学習を起こしてないことが図から読み取れる  
  
![image](https://user-images.githubusercontent.com/20613753/138594877-c3c2d188-eeef-4189-b4c9-87dee58c0b5e.png)  
↑左は未学習、真ん中は理想、右は過学習の図  
  
・ホールドアウト法  
データの数が少ない時は、検証用データがうまく作成されずに、検証がうまくいかないことがある。  
![image](https://user-images.githubusercontent.com/20613753/138595068-f523d31b-740d-4b83-a30d-a0f2f8a39b3c.png)  
  
・クロスバリデーション  
![image](https://user-images.githubusercontent.com/20613753/138595093-c15cc2cd-3312-4add-bc33-2b63474be1ed.png)  
  
・グリッドサーチ  
ハイパーパラメタの調整を行う手法  
![image](https://user-images.githubusercontent.com/20613753/138595134-d2645681-e587-4baf-993b-b8909a0ec644.png)  
  
### ロジスティック回帰モデル
分類問題に使用される  
  
識別的アプローチ  
→とあるクラスに割り当てられる確率を直接関数にする。ロジスティック回帰モデル。 
  
生成的アプローチ  
→ベイズの定理をかませて、割り当てられる確率を求める。GANなど。  

#### シグモイド関数
実数すべてを0と1の範囲につぶしている(変換している)イメージ。  
入力は実数、出力は必ず0~1。確率を表現。  
  
シグモイド関数の微分はシグモイド関数で書くことができる。  
![image](https://user-images.githubusercontent.com/20613753/138695424-c264029f-dee3-44b0-ab1c-5e21654c3398.png)  
keyword:合成関数の微分  

![image](https://user-images.githubusercontent.com/20613753/138695866-66c5dbe0-982b-445e-a93f-2e65a65cdc26.png)  
↑σがシグモイド関数。  

#### 尤度関数
尤度関数にlogがある理由は?  
→確率を何度も掛けると、数がとても小さくなってしまい、桁落ちが発生してしまう可能性が。  
→logで防ぐ  
  
![image](https://user-images.githubusercontent.com/20613753/138698225-d944eb88-bfe3-4d04-a1cb-bb2eeb3886a7.png)  
keyword:微分のchain rule

#### 確率的勾配降下法(SGD)
勾配降下法では、nが大きくなる(数百万とか)と、メモリに計算を載せられなくなる。  
よって、計算量を減らす工夫。  
![image](https://user-images.githubusercontent.com/20613753/138699011-17707a7f-09ed-4f49-8256-8be93534c2ae.png)  

### 主成分分析
