### 非線形な回帰

![image](https://user-images.githubusercontent.com/20613753/138592145-04aa7083-e1e0-4a44-8792-04ffb5d6b39a.png)  
↑の式は線形は式だが、xを非線形になるようなΦ(x)に変えれば良い！(Φ(x)=xに関する関数)  
(例えばxをxの二乗、sinxに置き換えるなど)  
  
(w(パラメータ)については非線形にせずとも、xの部分を非線形にすることで非線形な回帰を行うことができる)  
(wは求めるべき値だが、求める値は線形で良い)  
  
![image](https://user-images.githubusercontent.com/20613753/138592296-f9e1779c-c505-422d-9b21-875a848b2c1b.png)  
↑ここの部分を基底関数という  

![image](https://user-images.githubusercontent.com/20613753/138592578-49fcc078-5973-4261-82e3-ea79ba5fd702.png)  
↑非線形な回帰といえども、線形と同じ式で解ける(↓と同じ)  

![image](https://user-images.githubusercontent.com/20613753/138592601-54c1cc78-c1af-4421-8363-106dc9da2bfa.png)  
(テキスト26p)  

### 未学習と過学習
・未学習への対応　→　表現力の高いモデルを利用する

・過学習への対応　→　学習データの数を増やす、不要な変数を削除して表現力を抑止、正則化法(罰則化法)を利用する  
  
L2ノルムを使用→Ridge推定量  
L1ノルムを使用→Lasso推定量  
  
![image](https://user-images.githubusercontent.com/20613753/138594668-e7d34265-ee1d-455a-ac6c-5b0ad04caf1a.png)  
↑正則化することで、規定関数が増えても、過学習を起こしてないことが図から読み取れる  


